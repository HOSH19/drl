environment:
  grid_size: 15  # Smaller grid - easier to learn
  state_representation: "feature"
  initial_length: 3
  reward_food: 20.0  # Increased reward for eating food
  reward_death: -5.0  # Less harsh death penalty
  reward_step: 0.0  # NO step penalty - let agent explore
  reward_distance: 0.2  # Enable distance reward to guide agent toward food

dqn:
  learning_rate: 5e-4  # Slightly higher learning rate
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.1  # Keep more exploration (was 0.01)
  epsilon_decay: 0.9995  # Much slower decay - explore for longer
  replay_buffer_size: 50000
  batch_size: 32
  target_update_frequency: 500  # Update target network more frequently
  network: [64, 64]  # Simpler network
  activation: "relu"

ppo:
  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  update_epochs: 10
  batch_size: 64
  network: [128, 128, 64]
  activation: "relu"

training:
  algorithm: "dqn"
  total_episodes: 3000  # More episodes
  eval_frequency: 100
  save_frequency: 500
  update_frequency: 2  # Update more frequently
  log_dir: "./logs/snake"
  checkpoint_dir: "./checkpoints/snake"
  experiment_name: "snake_dqn_fixed"

evaluation:
  num_episodes: 10
  render: false
  save_videos: false
