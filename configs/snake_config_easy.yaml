environment:
  grid_size: 15  # Smaller grid for easier learning
  state_representation: "feature"
  initial_length: 3
  reward_food: 10.0
  reward_death: -5.0  # Less harsh penalty
  reward_step: 0.0  # No step penalty initially
  reward_distance: 0.1  # Enable distance reward to guide agent

dqn:
  learning_rate: 0.0001  # 1e-4
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.05  # Keep more exploration
  epsilon_decay: 0.998  # Slower decay (explore longer)
  replay_buffer_size: 50000  # Smaller buffer
  batch_size: 32  # Smaller batch
  target_update_frequency: 500  # Update more frequently
  network: [64, 64]  # Simpler network
  activation: "relu"

ppo:
  learning_rate: 0.0003  # 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  update_epochs: 10
  batch_size: 64
  network: [128, 128, 64]
  activation: "relu"

training:
  algorithm: "dqn"
  total_episodes: 1000  # More episodes
  eval_frequency: 50
  save_frequency: 200
  update_frequency: 4
  log_dir: "./logs/snake"
  checkpoint_dir: "./checkpoints/snake"
  experiment_name: "snake_dqn_easy"

evaluation:
  num_episodes: 10
  render: false
  save_videos: false
