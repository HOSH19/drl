{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake Game Deep RL Experiments\n",
    "\n",
    "This notebook provides interactive exploration and visualization of the Snake RL project.\n",
    "\n",
    "## Features:\n",
    "- Train and evaluate DQN and PPO agents\n",
    "- Visualize training progress\n",
    "- Compare different algorithms and hyperparameters\n",
    "- Watch trained agents play the game\n",
    "- Analyze policy behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from environments import SnakeEnv\n",
    "from agents import DQNAgent, PPODiscreteAgent\n",
    "from utils.training import MetricsTracker, evaluate_agent\n",
    "from utils.visualization import plot_training_curves, plot_algorithm_comparison\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../configs/snake_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Grid size: {config['environment']['grid_size']}\")\n",
    "print(f\"State representation: {config['environment']['state_representation']}\")\n",
    "print(f\"Algorithm: {config['training']['algorithm']}\")\n",
    "print(f\"Total episodes: {config['training']['total_episodes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = SnakeEnv(\n",
    "    grid_size=config['environment']['grid_size'],\n",
    "    state_representation=config['environment']['state_representation'],\n",
    "    initial_length=config['environment']['initial_length'],\n",
    "    reward_food=config['environment']['reward_food'],\n",
    "    reward_death=config['environment']['reward_death'],\n",
    "    reward_step=config['environment']['reward_step'],\n",
    "    reward_distance=config['environment']['reward_distance'],\n",
    "    render_mode=\"human\"  # Enable rendering\n",
    ")\n",
    "\n",
    "# Get state and action dimensions\n",
    "obs_space = env.observation_space\n",
    "if hasattr(obs_space, 'shape'):\n",
    "    state_shape = obs_space.shape\n",
    "else:\n",
    "    state_shape = (obs_space.n,)\n",
    "\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")\n",
    "\n",
    "# Test environment\n",
    "state, info = env.reset()\n",
    "print(f\"\\nInitial state shape: {state.shape if hasattr(state, 'shape') else len(state)}\")\n",
    "print(f\"Initial info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Test - Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random actions\n",
    "env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "while not done and steps < 100:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    # Render every 5 steps\n",
    "    if steps % 5 == 0:\n",
    "        env.render()\n",
    "\n",
    "print(f\"Random agent: Score={info.get('score', 0)}, Steps={steps}, Reward={total_reward:.2f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN agent\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    num_actions=env.action_space.n,\n",
    "    learning_rate=config['dqn']['learning_rate'],\n",
    "    gamma=config['dqn']['gamma'],\n",
    "    epsilon_start=config['dqn']['epsilon_start'],\n",
    "    epsilon_end=config['dqn']['epsilon_end'],\n",
    "    epsilon_decay=config['dqn']['epsilon_decay'],\n",
    "    replay_buffer_size=config['dqn']['replay_buffer_size'],\n",
    "    batch_size=config['dqn']['batch_size'],\n",
    "    target_update_frequency=config['dqn']['target_update_frequency'],\n",
    "    hidden_sizes=config['dqn']['network'],\n",
    "    activation=config['dqn']['activation'],\n",
    "    state_representation=config['environment']['state_representation'],\n",
    "    device=device,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"DQN agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (reduced episodes for notebook demo)\n",
    "metrics_tracker = MetricsTracker()\n",
    "total_episodes = 1000  # Reduced for demo\n",
    "update_frequency = 4\n",
    "\n",
    "print(f\"Training DQN for {total_episodes} episodes...\")\n",
    "\n",
    "for episode in tqdm(range(total_episodes), desc=\"Training\"):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    # Collect episode\n",
    "    while not done:\n",
    "        action = dqn_agent.act(state, deterministic=False)\n",
    "        next_state, reward, terminated, truncated, step_info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store transition\n",
    "        dqn_agent.store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # Train agent\n",
    "    if len(dqn_agent.replay_buffer) >= dqn_agent.batch_size:\n",
    "        if episode % update_frequency == 0:\n",
    "            metrics = dqn_agent.train_step()\n",
    "            metrics_tracker.record_episode(\n",
    "                reward=episode_reward,\n",
    "                score=info.get(\"score\", 0),\n",
    "                length=episode_length,\n",
    "                loss=metrics.get(\"loss\", None),\n",
    "                epsilon=metrics.get(\"epsilon\", None)\n",
    "            )\n",
    "        else:\n",
    "            metrics_tracker.record_episode(\n",
    "                reward=episode_reward,\n",
    "                score=info.get(\"score\", 0),\n",
    "                length=episode_length,\n",
    "                epsilon=dqn_agent.epsilon\n",
    "            )\n",
    "    else:\n",
    "        metrics_tracker.record_episode(\n",
    "            reward=episode_reward,\n",
    "            score=info.get(\"score\", 0),\n",
    "            length=episode_length,\n",
    "            epsilon=dqn_agent.epsilon\n",
    "        )\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        stats = metrics_tracker.get_statistics(window=100)\n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        print(f\"  Avg Reward: {stats.get('mean_reward', 0):.2f}\")\n",
    "        print(f\"  Avg Score: {stats.get('mean_score', 0):.2f}\")\n",
    "        print(f\"  Avg Length: {stats.get('mean_length', 0):.2f}\")\n",
    "        print(f\"  Epsilon: {stats.get('current_epsilon', 0):.3f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(metrics_tracker, window=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained agent\n",
    "eval_results = evaluate_agent(env, dqn_agent, num_episodes=10, deterministic=True)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "print(f\"Max Reward: {eval_results['max_reward']:.2f}\")\n",
    "print(f\"Mean Score: {eval_results['mean_score']:.2f} ± {eval_results['std_score']:.2f}\")\n",
    "print(f\"Max Score: {eval_results['max_score']:.2f}\")\n",
    "print(f\"Mean Length: {eval_results['mean_length']:.2f} ± {eval_results['std_length']:.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Watch Trained Agent Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch trained agent play\n",
    "env.render_mode = \"human\"\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "print(\"Watching trained agent play...\")\n",
    "print(\"Close the plot window to continue\")\n",
    "\n",
    "while not done and steps < 500:\n",
    "    action = dqn_agent.act(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, step_info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    steps += 1\n",
    "    \n",
    "    # Render every step\n",
    "    env.render()\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\nGame Over! Score: {info.get('score', 0)}, Steps: {steps}\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
