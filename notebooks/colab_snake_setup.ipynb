{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake Game Deep RL - Google Colab Setup\n",
    "\n",
    "This notebook sets up and runs the Snake RL project on Google Colab.\n",
    "\n",
    "## Features:\n",
    "- Free GPU access\n",
    "- Easy setup with one-click install\n",
    "- Save models to Google Drive\n",
    "- Visualize results inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive (Optional but Recommended)\n",
    "\n",
    "Mount your Google Drive to save models and results persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your drive path\n",
    "DRIVE_PATH = '/content/drive/MyDrive/drl_snake'  # Change this to your preferred path\n",
    "import os\n",
    "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "print(f\"Drive mounted! Checkpoint directory: {DRIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install gymnasium matplotlib pyyaml tqdm tensorboard\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Project Files\n",
    "\n",
    "**Option A: Upload from your local machine**\n",
    "- Use the file browser on the left to upload the entire `drl` folder\n",
    "- Or upload individual files as needed\n",
    "\n",
    "**Option B: Clone from GitHub (if you've pushed to GitHub)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Clone from GitHub (uncomment and modify if using)\n",
    "# !git clone https://github.com/yourusername/drl.git /content/drl\n",
    "# %cd /content/drl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Project Structure in Colab\n",
    "\n",
    "If you uploaded files, skip this. Otherwise, create the project structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set project directory (adjust if you uploaded to a different location)\n",
    "PROJECT_DIR = '/content/drl'  # Change if needed\n",
    "\n",
    "# Create project structure\n",
    "os.makedirs(f'{PROJECT_DIR}/src/environments', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/src/agents', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/src/networks', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/src/utils', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/src/experiments', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/configs', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/logs/snake', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/checkpoints/snake', exist_ok=True)\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, f'{PROJECT_DIR}/src')\n",
    "\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Python path updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upload Source Files\n",
    "\n",
    "**Important**: Upload all Python files from your local `src/` directory:\n",
    "- `src/environments/snake_env.py`\n",
    "- `src/environments/snake_renderer.py`\n",
    "- `src/environments/__init__.py`\n",
    "- `src/agents/dqn_agent.py`\n",
    "- `src/agents/ppo_discrete_agent.py`\n",
    "- `src/agents/__init__.py`\n",
    "- `src/networks/dqn_network.py`\n",
    "- `src/networks/__init__.py`\n",
    "- `src/utils/replay_buffer.py`\n",
    "- `src/utils/training.py`\n",
    "- `src/utils/visualization.py`\n",
    "- `src/utils/__init__.py`\n",
    "- `src/experiments/train_snake.py`\n",
    "- `src/experiments/evaluate_snake.py`\n",
    "- `src/experiments/__init__.py`\n",
    "\n",
    "**Or use the file browser to upload the entire `src/` folder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Config File\n",
    "\n",
    "Create the configuration file if you haven't uploaded it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_content = {\n",
    "    'environment': {\n",
    "        'grid_size': 20,\n",
    "        'state_representation': 'feature',  # 'grid', 'feature', or 'image'\n",
    "        'initial_length': 3,\n",
    "        'reward_food': 10.0,\n",
    "        'reward_death': -10.0,\n",
    "        'reward_step': -0.1,\n",
    "        'reward_distance': 0.0\n",
    "    },\n",
    "    'dqn': {\n",
    "        'learning_rate': 1e-4,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_end': 0.01,\n",
    "        'epsilon_decay': 0.995,\n",
    "        'replay_buffer_size': 100000,\n",
    "        'batch_size': 64,\n",
    "        'target_update_frequency': 1000,\n",
    "        'network': [128, 128, 64],\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    'ppo': {\n",
    "        'learning_rate': 3e-4,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'clip_epsilon': 0.2,\n",
    "        'value_coef': 0.5,\n",
    "        'entropy_coef': 0.01,\n",
    "        'max_grad_norm': 0.5,\n",
    "        'update_epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'network': [128, 128, 64],\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    'training': {\n",
    "        'algorithm': 'dqn',  # 'dqn' or 'ppo'\n",
    "        'total_episodes': 2000,  # Reduced for Colab demo\n",
    "        'eval_frequency': 100,\n",
    "        'save_frequency': 500,\n",
    "        'update_frequency': 4,\n",
    "        'log_dir': f'{PROJECT_DIR}/logs/snake',\n",
    "        'checkpoint_dir': f'{PROJECT_DIR}/checkpoints/snake',\n",
    "        'experiment_name': 'snake_dqn_colab'\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'num_episodes': 10,\n",
    "        'render': False,\n",
    "        'save_videos': False\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = f'{PROJECT_DIR}/configs/snake_config.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config_content, f)\n",
    "\n",
    "print(f\"Config file created at: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Imports\n",
    "\n",
    "Check that all modules can be imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "import os\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "# Import modules\n",
    "try:\n",
    "    from environments import SnakeEnv\n",
    "    from agents import DQNAgent, PPODiscreteAgent\n",
    "    from utils.training import MetricsTracker, evaluate_agent\n",
    "    from utils.visualization import plot_training_curves\n",
    "    print(\"✓ All modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"Please upload all source files first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Quick Test - Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Load config\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create environment\n",
    "env = SnakeEnv(\n",
    "    grid_size=config['environment']['grid_size'],\n",
    "    state_representation=config['environment']['state_representation'],\n",
    "    initial_length=config['environment']['initial_length'],\n",
    "    reward_food=config['environment']['reward_food'],\n",
    "    reward_death=config['environment']['reward_death'],\n",
    "    reward_step=config['environment']['reward_step'],\n",
    "    reward_distance=config['environment']['reward_distance']\n",
    ")\n",
    "\n",
    "# Test environment\n",
    "state, info = env.reset()\n",
    "print(f\"✓ Environment created successfully!\")\n",
    "print(f\"State shape: {state.shape if hasattr(state, 'shape') else len(state)}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Initial info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train Agent\n",
    "\n",
    "Train your agent. This will take some time depending on the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Get state shape\n",
    "obs_space = env.observation_space\n",
    "if hasattr(obs_space, 'shape'):\n",
    "    state_shape = obs_space.shape\n",
    "else:\n",
    "    state_shape = (obs_space.n,)\n",
    "\n",
    "# Create agent\n",
    "algorithm = config['training']['algorithm'].lower()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Algorithm: {algorithm.upper()}\")\n",
    "\n",
    "if algorithm == \"dqn\":\n",
    "    agent = DQNAgent(\n",
    "        state_shape=state_shape,\n",
    "        num_actions=env.action_space.n,\n",
    "        learning_rate=config['dqn']['learning_rate'],\n",
    "        gamma=config['dqn']['gamma'],\n",
    "        epsilon_start=config['dqn']['epsilon_start'],\n",
    "        epsilon_end=config['dqn']['epsilon_end'],\n",
    "        epsilon_decay=config['dqn']['epsilon_decay'],\n",
    "        replay_buffer_size=config['dqn']['replay_buffer_size'],\n",
    "        batch_size=config['dqn']['batch_size'],\n",
    "        target_update_frequency=config['dqn']['target_update_frequency'],\n",
    "        hidden_sizes=config['dqn']['network'],\n",
    "        activation=config['dqn']['activation'],\n",
    "        state_representation=config['environment']['state_representation'],\n",
    "        device=device,\n",
    "        seed=42\n",
    "    )\n",
    "elif algorithm == \"ppo\":\n",
    "    agent = PPODiscreteAgent(\n",
    "        state_shape=state_shape,\n",
    "        num_actions=env.action_space.n,\n",
    "        learning_rate=config['ppo']['learning_rate'],\n",
    "        gamma=config['ppo']['gamma'],\n",
    "        gae_lambda=config['ppo']['gae_lambda'],\n",
    "        clip_epsilon=config['ppo']['clip_epsilon'],\n",
    "        value_coef=config['ppo']['value_coef'],\n",
    "        entropy_coef=config['ppo']['entropy_coef'],\n",
    "        max_grad_norm=config['ppo']['max_grad_norm'],\n",
    "        update_epochs=config['ppo']['update_epochs'],\n",
    "        batch_size=config['ppo']['batch_size'],\n",
    "        hidden_sizes=config['ppo']['network'],\n",
    "        activation=config['ppo']['activation'],\n",
    "        state_representation=config['environment']['state_representation'],\n",
    "        device=device,\n",
    "        seed=42\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "\n",
    "print(f\"✓ Agent created: {type(agent).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "metrics_tracker = MetricsTracker()\n",
    "total_episodes = config['training']['total_episodes']\n",
    "eval_frequency = config['training']['eval_frequency']\n",
    "save_frequency = config['training']['save_frequency']\n",
    "update_frequency = config['training']['update_frequency']\n",
    "\n",
    "print(f\"Starting training for {total_episodes} episodes...\")\n",
    "print(f\"Evaluation every {eval_frequency} episodes\")\n",
    "print(f\"Saving checkpoint every {save_frequency} episodes\")\n",
    "\n",
    "best_score = -np.inf\n",
    "\n",
    "for episode in tqdm(range(total_episodes), desc=\"Training\"):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    # Collect episode\n",
    "    while not done:\n",
    "        if algorithm == \"dqn\":\n",
    "            action = agent.act(state, deterministic=False)\n",
    "            next_state, reward, terminated, truncated, step_info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "        else:  # ppo\n",
    "            action, log_prob, value = agent.act(state, deterministic=False)\n",
    "            next_state, reward, terminated, truncated, step_info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.store_transition(state, action, reward, log_prob, value, done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # Train agent\n",
    "    if algorithm == \"dqn\":\n",
    "        if len(agent.replay_buffer) >= agent.batch_size:\n",
    "            if episode % update_frequency == 0:\n",
    "                metrics = agent.train_step()\n",
    "                metrics_tracker.record_episode(\n",
    "                    reward=episode_reward,\n",
    "                    score=info.get(\"score\", 0),\n",
    "                    length=episode_length,\n",
    "                    loss=metrics.get(\"loss\", None),\n",
    "                    epsilon=metrics.get(\"epsilon\", None)\n",
    "                )\n",
    "            else:\n",
    "                metrics_tracker.record_episode(\n",
    "                    reward=episode_reward,\n",
    "                    score=info.get(\"score\", 0),\n",
    "                    length=episode_length,\n",
    "                    epsilon=agent.epsilon\n",
    "                )\n",
    "        else:\n",
    "            metrics_tracker.record_episode(\n",
    "                reward=episode_reward,\n",
    "                score=info.get(\"score\", 0),\n",
    "                length=episode_length,\n",
    "                epsilon=agent.epsilon\n",
    "            )\n",
    "    else:  # ppo\n",
    "        if episode % update_frequency == 0 and len(agent.states) > 0:\n",
    "            metrics = agent.train_step()\n",
    "            metrics_tracker.record_episode(\n",
    "                reward=episode_reward,\n",
    "                score=info.get(\"score\", 0),\n",
    "                length=episode_length,\n",
    "                loss=metrics.get(\"loss\", None)\n",
    "            )\n",
    "        else:\n",
    "            metrics_tracker.record_episode(\n",
    "                reward=episode_reward,\n",
    "                score=info.get(\"score\", 0),\n",
    "                length=episode_length\n",
    "            )\n",
    "    \n",
    "    # Evaluation\n",
    "    if (episode + 1) % eval_frequency == 0:\n",
    "        eval_results = evaluate_agent(env, agent, num_episodes=5, deterministic=True)\n",
    "        stats = metrics_tracker.get_statistics(window=100)\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        print(f\"  Recent Avg Reward: {stats.get('mean_reward', 0):.2f}\")\n",
    "        print(f\"  Recent Avg Score: {stats.get('mean_score', 0):.2f}\")\n",
    "        print(f\"  Recent Avg Length: {stats.get('mean_length', 0):.2f}\")\n",
    "        print(f\"  Eval Avg Score: {eval_results['mean_score']:.2f}\")\n",
    "        print(f\"  Eval Max Score: {eval_results['max_score']:.2f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if eval_results['mean_score'] > best_score:\n",
    "            best_score = eval_results['mean_score']\n",
    "            checkpoint_path = f'{PROJECT_DIR}/checkpoints/snake/best_model.pth'\n",
    "            agent.save(checkpoint_path)\n",
    "            print(f\"  ✓ Saved best model (score: {best_score:.2f})\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (episode + 1) % save_frequency == 0:\n",
    "        checkpoint_path = f'{PROJECT_DIR}/checkpoints/snake/checkpoint_ep{episode+1}.pth'\n",
    "        agent.save(checkpoint_path)\n",
    "        metrics_tracker.save(f'{PROJECT_DIR}/checkpoints/snake/metrics.json')\n",
    "        print(f\"  ✓ Saved checkpoint at episode {episode + 1}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plot_training_curves(metrics_tracker, window=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained agent\n",
    "eval_results = evaluate_agent(env, agent, num_episodes=10, deterministic=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Evaluation Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "print(f\"Max Reward: {eval_results['max_reward']:.2f}\")\n",
    "print(f\"Mean Score: {eval_results['mean_score']:.2f} ± {eval_results['std_score']:.2f}\")\n",
    "print(f\"Max Score: {eval_results['max_score']:.2f}\")\n",
    "print(f\"Mean Length: {eval_results['mean_length']:.2f} ± {eval_results['std_length']:.2f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to Google Drive\n",
    "if 'DRIVE_PATH' in globals():\n",
    "    import shutil\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = f'{DRIVE_PATH}/snake_final_model.pth'\n",
    "    agent.save(final_model_path)\n",
    "    print(f\"✓ Final model saved to: {final_model_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = f'{DRIVE_PATH}/metrics.json'\n",
    "    metrics_tracker.save(metrics_path)\n",
    "    print(f\"✓ Metrics saved to: {metrics_path}\")\n",
    "    \n",
    "    # Copy best model if it exists\n",
    "    best_model_local = f'{PROJECT_DIR}/checkpoints/snake/best_model.pth'\n",
    "    if os.path.exists(best_model_local):\n",
    "        best_model_drive = f'{DRIVE_PATH}/snake_best_model.pth'\n",
    "        shutil.copy(best_model_local, best_model_drive)\n",
    "        print(f\"✓ Best model copied to: {best_model_drive}\")\n",
    "else:\n",
    "    print(\"Google Drive not mounted. Models saved locally.\")\n",
    "    print(f\"Note: Local files will be deleted when Colab session ends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Download Results (Optional)\n",
    "\n",
    "If you didn't mount Google Drive, download the model files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files\n",
    "from google.colab import files\n",
    "\n",
    "# Download best model\n",
    "if os.path.exists(f'{PROJECT_DIR}/checkpoints/snake/best_model.pth'):\n",
    "    files.download(f'{PROJECT_DIR}/checkpoints/snake/best_model.pth')\n",
    "\n",
    "# Download metrics\n",
    "if os.path.exists(f'{PROJECT_DIR}/checkpoints/snake/metrics.json'):\n",
    "    files.download(f'{PROJECT_DIR}/checkpoints/snake/metrics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Colab Usage\n",
    "\n",
    "1. **GPU Runtime**: Go to Runtime → Change runtime type → Select GPU (T4 or better)\n",
    "\n",
    "2. **Session Limits**: \n",
    "   - Free Colab: ~12 hours max, may disconnect\n",
    "   - Save checkpoints frequently\n",
    "   - Use Google Drive for persistence\n",
    "\n",
    "3. **Memory Management**:\n",
    "   - Reduce `batch_size` if you get OOM errors\n",
    "   - Reduce `replay_buffer_size` for DQN\n",
    "   - Use smaller network sizes\n",
    "\n",
    "4. **Faster Training**:\n",
    "   - Use GPU runtime\n",
    "   - Reduce `total_episodes` for quick tests\n",
    "   - Use `feature` state representation (faster than `image`)\n",
    "\n",
    "5. **Resume Training**:\n",
    "   - Load checkpoint: `agent.load('path/to/checkpoint.pth')`\n",
    "   - Continue training from saved episode\n",
    "\n",
    "6. **TensorBoard in Colab**:\n",
    "   ```python\n",
    "   %load_ext tensorboard\n",
    "   %tensorboard --logdir /content/drl/logs/snake\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
